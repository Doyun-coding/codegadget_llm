# static_train_codebart_gpu_v3.py
import os
import numpy as np
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
)
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import inspect

MODEL = "microsoft/codebert-base"

def main():
    # ì•ˆì „ ì˜µì…˜: í† í¬ë‚˜ì´ì € ë©€í‹°ìŠ¤ë ˆë“œ ê²½ê³ /êµì°© ë°©ì§€
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    # ë””ë²„ê·¸ìš©(ë©ˆì¶¤ ì¶”ì ) â€” ëŠë ¤ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ í•„ìš” ì‹œë§Œ ì£¼ì„ í•´ì œ
    # os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

    use_cuda = torch.cuda.is_available()
    if use_cuda:
        print(f"[INFO] CUDA available. Using GPU: {torch.cuda.get_device_name(0)}")
    else:
        print("[INFO] No CUDA detected â€” training on CPU.")

    tokenizer = AutoTokenizer.from_pretrained(MODEL)

    def tokenize_fn(batch):
        # 1660 SUPER: 256 í† í°ì´ ì†ë„/í’ˆì§ˆ ê· í˜•. ë” ë¹ ë¥´ê²Œ í•˜ë ¤ë©´ 192/128ë¡œ ì¤„ì—¬ë„ ë¨
        return tokenizer(batch["text"], truncation=True, max_length=256)

    dataset = load_dataset(
        "json",
        data_files={
            "train": "./data/static/v3/juliet_codebert_dataset/train.jsonl",
            "validation": "./data/static/v3/juliet_codebert_dataset/validation.jsonl",
            "test": "./data/static/v3/juliet_codebert_dataset/test.jsonl",
        },
    )

    def fix_label(example):
        example["label"] = int(example["label"])
        return example

    # ì‚¬ì „ ê°€ê³µ(ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤: ìœˆë„ìš° ì•ˆì „)
    dataset = dataset.map(fix_label)
    dataset = dataset.map(tokenize_fn, batched=True)
    dataset = dataset.map(lambda x: {"labels": x["label"]})
    dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

    # ë™ì  íŒ¨ë”©(í…ì„œì½”ì–´ ì •ë ¬ìš© pad_to_multiple_of=8)
    collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)

    # ë¼ë²¨ ë§µí•‘(ì €ì¥/ì¶”ë¡  ì‹œ ìœ ìš©)
    id2label = {0: "nonvuln", 1: "vuln"}
    label2id = {"nonvuln": 0, "vuln": 1}

    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL,
        num_labels=2,
        id2label=id2label,
        label2id=label2id,
    )

    # í´ë˜ìŠ¤ ë¶ˆê· í˜• ê°€ì¤‘ì¹˜
    train_labels = np.array(dataset["train"]["labels"])
    n0 = int((train_labels == 0).sum())
    n1 = int((train_labels == 1).sum())
    class_weights = None
    if n0 > 0 and n1 > 0:
        w0 = (n0 + n1) / (2.0 * n0)
        w1 = (n0 + n1) / (2.0 * n1)
        class_weights = torch.tensor([w0, w1])

    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        preds = np.argmax(logits, axis=-1)
        return {
            "accuracy": accuracy_score(labels, preds),
            "precision": precision_score(labels, preds, zero_division=0),
            "recall": recall_score(labels, preds, zero_division=0),
            "f1": f1_score(labels, preds, zero_division=0),
        }

    # âš  ìœˆë„ìš° ì•ˆì „ + ì •ì§€ íšŒí”¼ ì„¸íŒ…
    training_args = TrainingArguments(
        output_dir="./static-codebert",
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=2,  # ì²´í¬í¬ì¸íŠ¸ 2ê°œë§Œ ë³´ê´€
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        greater_is_better=True,

        learning_rate=2e-5,
        weight_decay=0.01,
        warmup_ratio=0.06,
        num_train_epochs=4,

        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,

        optim="adamw_torch",
        report_to="none",

        # Windows ì•ˆì •ì„±ìš©(ìœˆë„ìš°ë©´ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”; ë§¥/ë¦¬ëˆ…ìŠ¤ëŠ” ìƒëµí•´ë„ ë¬´ë°©)
        dataloader_num_workers=0,
        dataloader_pin_memory=False,

        # í˜¼í•©ì •ë°€: CUDAë§Œ Trueë¡œ, MPS/CPUëŠ” False
        fp16=torch.cuda.is_available(),
        # Ampere ì´ìƒì´ë©´ bf16ë„ ê³ ë ¤ (ì›í•˜ë©´ í™œì„±í™”)
        # bf16=(torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8),
    )

    class WeightedTrainer(Trainer):
        def __init__(self, class_weights_tensor=None, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self.class_weights = class_weights_tensor

        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
            labels = inputs.get("labels")
            outputs = model(**inputs)
            logits = outputs.logits if hasattr(outputs, "logits") else outputs.get("logits")
            if self.class_weights is not None:
                cw = self.class_weights.to(logits.device)
                loss_fct = torch.nn.CrossEntropyLoss(weight=cw)
            else:
                loss_fct = torch.nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))
            return (loss, outputs) if return_outputs else loss

    # HF ë²„ì „ í˜¸í™˜(í† í¬ë‚˜ì´ì € ì „ë‹¬ ê²½ê³  ì œê±°)
    trainer_kwargs = dict(
        model=model,
        args=training_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["validation"],   # eval_strategy="no"ë¼ í•™ìŠµ ì¤‘ì—” ì‹¤í–‰ ì•ˆ ë¨
        compute_metrics=compute_metrics,
        data_collator=collator,
        class_weights_tensor=class_weights,
    )
    if "processing_class" in inspect.signature(Trainer.__init__).parameters:
        trainer_kwargs["processing_class"] = tokenizer
    else:
        trainer_kwargs["tokenizer"] = tokenizer

    trainer = WeightedTrainer(**trainer_kwargs)

    print(">>> start training")
    trainer.train()

    # ====== ğŸ‘‡ í•™ìŠµ ê²°ê³¼ ì €ì¥ (ëª¨ë¸ + í† í¬ë‚˜ì´ì € + ìƒíƒœ) ======
    save_dir = training_args.output_dir  # "./static-codebert"
    os.makedirs(save_dir, exist_ok=True)
    trainer.save_model(save_dir)               # ëª¨ë¸ ê°€ì¤‘ì¹˜/êµ¬ì„± ì €ì¥
    tokenizer.save_pretrained(save_dir)        # í† í¬ë‚˜ì´ì € íŒŒì¼ ì €ì¥
    trainer.save_state()                       # (ì„ íƒ) optimizer/scheduler state ë“± ì €ì¥
    print(f"[SAVED] Model & tokenizer saved to: {save_dir}")

    print(">>> evaluate on test")
    res = trainer.evaluate(dataset["test"])
    print("Test eval:", res)

if __name__ == "__main__":
    main()
